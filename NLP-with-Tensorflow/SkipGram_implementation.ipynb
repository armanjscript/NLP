{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "%matplotlib inline\n",
        "!pip install adjustText\n",
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from adjustText import adjust_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uRT4CSp7NLu",
        "outputId": "f05a7ede-0051-4177-cb4c-8d27c61ec3a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adjustText\n",
            "  Downloading adjustText-1.2.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from adjustText) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from adjustText) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from adjustText) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n",
            "Downloading adjustText-1.2.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: adjustText\n",
            "Successfully installed adjustText-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url, data_dir):\n",
        "\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "  filepath = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "  if not os.path.exists(filepath):\n",
        "    print('Downloading file ...')\n",
        "    filename, _ = urlretrieve(url, filepath)\n",
        "  else:\n",
        "    print('File already exists')\n",
        "\n",
        "  extract_path = os.path.join(data_dir, 'bbc')\n",
        "  if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(\n",
        "        os.path.join(data_dir, 'bbc-fulltext.zip'),\n",
        "        'r'\n",
        "    ) as zipf:\n",
        "      zipf.extractall(data_dir)\n",
        "  else:\n",
        "    print('bbc-fulltext.zip has already been extracted')"
      ],
      "metadata": {
        "id": "kGF38Wg2LpAp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MM3KiOtN3Ai",
        "outputId": "ed554df4-1ff4-48d3-f78a-16d8ae34c1b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "\n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "\n",
        "    print(\"Reading files\")\n",
        "\n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "\n",
        "        for fi, f in enumerate(files):\n",
        "\n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "\n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "\n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "\n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "\n",
        "                    story.append(row.strip())\n",
        "\n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)\n",
        "                # Add that to the list\n",
        "                news_stories.append(story)\n",
        "\n",
        "        print('', end='\\r')\n",
        "\n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "\n",
        "\n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "id": "Rf47cU9DOTGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80869722-eeb5-49d6-8443-3f43934bf972"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 121.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Labour in constituency race row  Labour's choice o\n",
            "Example words (end):  tion and low unemployment may be coming to an end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower = True,\n",
        "    split = ' '\n",
        ")\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbtd9uia8ab2",
        "outputId": "e6d1c183-f37b-49e7-ffc5-bd81d3a5c645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(tokenizer.word_index.items()) + 1\n",
        "print(f\"Vocabulary size: {n_vocab}\")\n",
        "\n",
        "print(\"\\nWords at the top\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
        "print(\"\\nWords at the bottom\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwqaP2M9e8n",
        "outputId": "97736bc7-4c6f-4980-844a-141a0f4eddcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32360\n",
            "\n",
            "Words at the top\n",
            "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
            "\n",
            "Words at the bottom\n",
            "\t {'dz': 32350, 'bernd': 32351, 'weidensteiner': 32352, 'sinn': 32353, 'obstruct': 32354, 'avery': 32355, 'shenfeld': 32356, 'cibc': 32357, 'sohn': 32358, 'geopolitics': 32359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words = 15000,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', oov_token='',\n",
        ")\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy6VS5Xe-SuN",
        "outputId": "ad4de616-35ab-4bc3-d77d-70c2d40dfd05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original: {news_stories[0][:100]}\")\n",
        "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go3LBI3P_Dzv",
        "outputId": "8b8f1284-4e0d-4c55-c7db-7ac771fd0e51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Labour in constituency race row  Labour's choice of a white candidate for one of the UK's most multi\n",
            "Sequence IDs: [126, 7, 2795, 630, 1016, 787, 661, 4, 6, 1094, 2432, 8, 50, 4, 2, 945, 113, 2391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting all articles to word ID sequences**"
      ],
      "metadata": {
        "id": "GiDBLdfYAHaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ],
      "metadata": {
        "id": "XDmOxn-KADWW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating skip-grams from the corpus**"
      ],
      "metadata": {
        "id": "LrgWbIVxAXVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word_ids = news_sequences[0][:5]\n",
        "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
        "print(f\"Sample phrase: {sample_phrase}\")\n",
        "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
        "\n",
        "window_size = 1 # How many words to consider left and right.\n",
        "\n",
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=n_vocab,\n",
        "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample skip-grams\")\n",
        "\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZS3zFAPAZ1_",
        "outputId": "72f7a3b2-5ac2-45f7-cefe-b2a6053ad927"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample phrase: labour in constituency race row\n",
            "Sample word IDs: [126, 7, 2795, 630, 1016]\n",
            "\n",
            "Sample skip-grams\n",
            "\tInput: [126, 7] (['labour', 'in']) / Label: 1\n",
            "\tInput: [7, 126] (['in', 'labour']) / Label: 1\n",
            "\tInput: [7, 2795] (['in', 'constituency']) / Label: 1\n",
            "\tInput: [2795, 7] (['constituency', 'in']) / Label: 1\n",
            "\tInput: [2795, 630] (['constituency', 'race']) / Label: 1\n",
            "\tInput: [630, 2795] (['race', 'constituency']) / Label: 1\n",
            "\tInput: [630, 1016] (['race', 'row']) / Label: 1\n",
            "\tInput: [1016, 630] (['row', 'race']) / Label: 1\n",
            "\tInput: [126, 14684] (['labour', 'alabama']) / Label: 0\n",
            "\tInput: [630, 6407] (['race', 'totalling']) / Label: 0\n",
            "\tInput: [1016, 12663] (['row', 'compress']) / Label: 0\n",
            "\tInput: [7, 16213] (['in', 'â£947m']) / Label: 0\n",
            "\tInput: [2795, 21465] (['constituency', \"mackenzie's\"]) / Label: 0\n",
            "\tInput: [2795, 12498] (['constituency', 'fog']) / Label: 0\n",
            "\tInput: [7, 27145] (['in', 'pavlikowsky']) / Label: 0\n",
            "\tInput: [630, 23255] (['race', 'centimetre']) / Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating negative candidates**"
      ],
      "metadata": {
        "id": "x2UiRuokBnY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
        "    window_size=window_size, negative_samples=0, shuffle=False,\n",
        ")\n",
        "\n",
        "inputs, labels = np.array(inputs), np.array(labels)\n",
        "\n",
        "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
        "    # A true context word that appears in the context of the target\n",
        "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
        "    num_true=1, # number of true words per example\n",
        "    num_sampled=10,\n",
        "    unique=True,\n",
        "    range_max=n_vocab,\n",
        "    name=\"negative_sampling\"\n",
        ")\n",
        "\n",
        "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
        "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
        "print(f\"true_expected_count: {true_expected_count}\")\n",
        "print(f\"sampled_expected_count: {sampled_expected_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRTY43xmBdim",
        "outputId": "953128c1-1442-4de6-9228-748dd7ce1b92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample: [[7]]\n",
            "Negative samples: [   39  3099     3   336 20913     7    83   717  8621   109]\n",
            "true_expected_count: [[0.11341967]]\n",
            "sampled_expected_count: [2.37778574e-02 3.10580333e-04 2.14877039e-01 2.85319984e-03\n",
            " 4.60424235e-05 1.13419674e-01 1.13960411e-02 1.34022883e-03\n",
            " 1.11679241e-04 8.71457718e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using tf.nn.sampled_softmax_loss()**"
      ],
      "metadata": {
        "id": "WxhRBcbM_tON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
        "print(sampling_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyg2K-SD_aIO",
        "outputId": "6c3b2168-6324-41a3-e654-68310948ab52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating data (positive + negative candidates)**"
      ],
      "metadata": {
        "id": "1bh2_mt7_3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
        "\n",
        "    rand_sequence_ids = np.arange(len(sequences))\n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "\n",
        "    for si in rand_sequence_ids:\n",
        "\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequences[si],\n",
        "            vocabulary_size=vocab_size,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0.0,\n",
        "            shuffle=False,\n",
        "            sampling_table=sampling_table,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        targets, contexts, labels = [], [], []\n",
        "\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "\n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "              true_classes=context_class,\n",
        "              num_true=1,\n",
        "              num_sampled=negative_samples,\n",
        "              unique=True,\n",
        "              range_max=vocab_size,\n",
        "              name=\"negative_sampling\")\n",
        "\n",
        "            # Build context and label vectors (for one target word)\n",
        "            context = tf.concat(\n",
        "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
        "                axis=0\n",
        "            )\n",
        "\n",
        "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
        "\n",
        "            # Append each element from the training example to global lists.\n",
        "            targets.extend([target_word]*(negative_samples+1))\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "\n",
        "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
        "\n",
        "        assert contexts.shape[0] == targets.shape[0]\n",
        "        assert contexts.shape[0] == labels.shape[0]\n",
        "\n",
        "        # If seed is not provided generate a random one\n",
        "        if not seed:\n",
        "            seed = random.randint(0, 10e6)\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(contexts)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(targets)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(labels)\n",
        "\n",
        "\n",
        "        for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
        "            yield (\n",
        "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
        "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
        "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
        "\n",
        "\n",
        "news_skip_gram_gen = skip_gram_data_generator(\n",
        "    news_sequences, 4, 10, 5, n_vocab\n",
        ")\n",
        "\n",
        "for btc, bl in news_skip_gram_gen:\n",
        "\n",
        "    print(btc)\n",
        "    print(bl)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ6X_pzK_0g-",
        "outputId": "ec742cfd-6135-46db-e945-e6d780db1df3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 1407,  3820,  8291,  2936,  3354, 14185,  8145,  5752,    26,\n",
            "        5767]), array([    2,  3592, 24048, 16115,  3820,  1769,   431,     6,    28,\n",
            "         191]))\n",
            "[0 0 0 0 1 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip -Gram Algorithm**"
      ],
      "metadata": {
        "id": "JRtIxnXJB19O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Hyperparameters\n",
        "\n",
        "batch_size = 4096 # Data points in a single batch\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "window_size=1 # We use a window size of n on either side of target word\n",
        "negative_samples = 4 # Number of negative samples generated per example\n",
        "\n",
        "epochs = 5 # Number of epochs to train for\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 250\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words as well\n",
        "np.random.seed(54321)\n",
        "random.seed(54321)\n",
        "\n",
        "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_term_ids = np.append(\n",
        "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
        "    axis=0\n",
        ")"
      ],
      "metadata": {
        "id": "-AaqeCSvB6H9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Model\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Inputs - skipgrams() function outputs target, context in that order\n",
        "# we will use the same order\n",
        "input_1 = tf.keras.layers.Input(shape=(), name='target')\n",
        "input_2 = tf.keras.layers.Input(shape=(), name='context')\n",
        "\n",
        "# Two embeddings layers are used one for the context and one for the target\n",
        "context_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
        ")\n",
        "target_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
        ")\n",
        "\n",
        "# Lookup outputs of the embedding layers\n",
        "target_out = target_embedding_layer(input_1)\n",
        "context_out = context_embedding_layer(input_2)\n",
        "\n",
        "# Computing the dot product between the two\n",
        "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
        "\n",
        "# Defining the model\n",
        "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='skip_gram_model')\n",
        "\n",
        "# Compiling the model\n",
        "skip_gram_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam'\n",
        ")\n",
        "\n",
        "skip_gram_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ouBjK2XcCZe-",
        "outputId": "726c4c3b-2421-400c-b11e-50c724244a9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"skip_gram_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"skip_gram_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ context (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m4,142,080\u001b[0m │ context[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m4,142,080\u001b[0m │ target[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ context_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ target_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ context (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ context[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ target[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ target_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Word Similarities\n",
        "\n",
        "class ValidationCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
        "\n",
        "        self.valid_term_ids = valid_term_ids\n",
        "        self.model_with_embeddings = model_with_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\" Validation logic \"\"\"\n",
        "\n",
        "        # We will use context embeddings to get the most similar words\n",
        "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
        "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
        "\n",
        "        # Get the embeddings corresponding to valid_term_ids\n",
        "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
        "\n",
        "        # Compute the similarity between valid_term_ids and all the embeddings\n",
        "        # V x d (d x D) => V x D\n",
        "        top_k = 5 # Top k items will be displayed\n",
        "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
        "\n",
        "        # Invert similarity matrix to negative\n",
        "        # Ignore the first one because that would be the same word as the probe word\n",
        "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "        # Print the output\n",
        "        for i, term_id in enumerate(valid_term_ids):\n",
        "\n",
        "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
        "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
        "\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "Rl6nQdWzDc1W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the Skip-Gram Algorithm\n",
        "\n",
        "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
        "\n",
        "for ei in range(epochs):\n",
        "\n",
        "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
        "\n",
        "    news_skip_gram_gen = skip_gram_data_generator(\n",
        "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
        "    )\n",
        "\n",
        "    skip_gram_model.fit(\n",
        "        news_skip_gram_gen, epochs=1,\n",
        "        callbacks=skipgram_validation_callback,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1dQptTvGKIm",
        "outputId": "0cbfbc74-8922-44ce-ea6f-c7ef73e9a1f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 started\n",
            "   2233/Unknown \u001b[1m333s\u001b[0m 148ms/step - loss: 0.6367"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "election: attorney, anticipated, chong, tell, vido\n",
            "months: weeks, years, complaints, vote', affected\n",
            "with: 121, shadowing, statistical, beverley, against\n",
            "you: they, we, sure, what, do\n",
            "were: then, lost, only, mean, nothing\n",
            "win: likely, issue, serve, lot, trying\n",
            "those: doing, trying, lot, affected, mainstream\n",
            "music: way, trying, lot, concerned\n",
            "also: still, they, nothing, able, already\n",
            "international: came, religious, flooded, programmes, miscellaneous\n",
            "best: end, responsibility, surprised, forming\n",
            "down: able, can't, starting, now, machine\n",
            "too: taken, put, escape, fines, lot\n",
            "some: trying, case, side's, account\n",
            "through: me, boosted, possible, laptop, accusations\n",
            "mr: tony, said, gordon, charles, michael\n",
            "fast: understand, remake, affected, barring, take\n",
            "road: jameson, forsyth, proud, lifted, divided\n",
            "bush: replay, background, weeks', tally, moguls\n",
            "significant: according, kind, trying, spider, failure\n",
            "reached: trying, prepare, status, keen, heading\n",
            "serious: driven, others, see, jinshajiang, used\n",
            "wins: started, possibility, think, added, concerned\n",
            "risk: used, decided, happens, find, doing\n",
            "operating: collaboration, maybe, puts, edgware, force\n",
            "actually: idea, amicus', able, matter, switzerland\n",
            "deutsche: objected, insulted, prolong, illustrate, knows\n",
            "couple: kind, concerned, switzerland, lot, decided\n",
            "asylum: clocked, role, asked, dutchman, lead\n",
            "allowed: able, saves, find, likes, space\n",
            "davis: supremo, conservatives, rest, find, accompanied\n",
            "mini: built, switzerland, lot, trying, curb\n",
            "\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 148ms/step - loss: 0.6367\n",
            "Epoch: 2/5 started\n",
            "   2233/Unknown \u001b[1m355s\u001b[0m 159ms/step - loss: 0.4674election: attorney, impending, bmw, anticipated, drafted\n",
            "months: weeks, years, days, hours, number\n",
            "with: between, one's, bright, viotti's, recipients\n",
            "you: they, we, don't, wouldn't, didn't\n",
            "were: are, am, been, have, they're\n",
            "win: divide, lead, eased, switzerland, penalties\n",
            "those: 384, rocket, bother, abandoning, bysshe\n",
            "music: image, ofsted, arts, 2006', overture\n",
            "also: already, still, never, often, previously\n",
            "international: policy, great, meeting, rugby, land\n",
            "best: westlife, market's, jammed, joynt, chandratillake\n",
            "down: surge, roused, â£60bn, prototypes, mobiles\n",
            "too: voicing, score, retrieving, diatribe, liu\n",
            "some: ifpi, pensioners', pixies', offshore, disappear\n",
            "through: thirlwell, enthusiasts, 4x200m, mantilla, convince\n",
            "mr: tony, gordon, said, michael, jack\n",
            "fast: alerts, ramp, reissued, fingerprints, advancement\n",
            "road: academics, federation's, hitzlsperger's, they've, yard's\n",
            "bush: bush's, advocated, yellow, riot, crowther\n",
            "significant: according, thirty, presidency, alphabets, yugansk\n",
            "reached: 144, halt, directconnect, loudmouth, check\n",
            "serious: leveaux, rounding, â£104m, 1980\n",
            "wins: accountancy, imposed, wifi, crept, saviour\n",
            "risk: kind, lent, framework, credited, phpbb\n",
            "operating: transport, improving, another', dictate, balancing\n",
            "actually: demonstrations, thus, persuade, bloodthirsty, uniquely\n",
            "deutsche: initiated, reserve, best, german, union\n",
            "couple: kind, 1980, maldives, refusing, emergence\n",
            "asylum: genuinely, muna, avid, whelan, buzz\n",
            "allowed: apologise, able, done, bowers, leaning\n",
            "davis: heineken, fa, hopman, rascal, fletcher\n",
            "mini: kooyong, embattled, watchers, bears, uncompromising\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 159ms/step - loss: 0.4673\n",
            "Epoch: 3/5 started\n",
            "   2233/Unknown \u001b[1m378s\u001b[0m 169ms/step - loss: 0.4153election: attorney, anticipated, poster, november's, impending\n",
            "months: weeks, years, days, number, month\n",
            "with: one's, between, atlantic, fenice, viotti's\n",
            "you: didn't, don't, we, they, why\n",
            "were: are, am, been, they're, being\n",
            "win: lead, defeats, decade, eased, divide\n",
            "those: dre, others, browsers, exactly, rfu\n",
            "music: surround, walt, fearsome, contained, arts\n",
            "also: already, previously, often, repeatedly, never\n",
            "international: meeting, policy, 200m, world, rugby\n",
            "best: westlife, feature, fastest, familiar, counterparts\n",
            "down: together, back, comments, she's, penalty\n",
            "too: very, quite, enough, so, bit\n",
            "some: spice, eli, taxi, restrictions, compressed\n",
            "through: verbal, humans, anonymously, 'should, panic\n",
            "mr: tony, gordon, gerhard, malcolm, ken\n",
            "fast: popularity, undertaking, minus, alerts, wireless\n",
            "road: providing, detailed, monsters, your, reconstruction\n",
            "bush: bush's, george, pozzebon, chabal, gardner\n",
            "significant: reasonable, key, unprecedented, current, basic\n",
            "reached: fortuitous, idris, 144, anglicans, notorious\n",
            "serious: deeper, khatami's, rambo, awful, spamming\n",
            "wins: angry, launch, chunk, undertaking, suffered\n",
            "risk: plenty, parton, denunciation, homecoming, cadmium\n",
            "operating: astonishing, retains, elgindy's, technology, bulbs\n",
            "actually: we'll, you're, wasn't, shoppers, desperately\n",
            "deutsche: austria, central, austria's, english, takeover\n",
            "couple: kind, dermot, 86m, forsythe's, tainted\n",
            "asylum: genuinely, quiksilver, mate, kashmir, awaiting\n",
            "allowed: apologise, able, discover, pledged, fights\n",
            "davis: o'connell, uefa, fa, parisse, sean\n",
            "mini: kooyong, format, lesotho, embattled, avoided\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 169ms/step - loss: 0.4153\n",
            "Epoch: 4/5 started\n",
            "   2233/Unknown \u001b[1m390s\u001b[0m 175ms/step - loss: 0.3820election: attorney, november's, motors, anticipated, poster\n",
            "months: weeks, years, days, hours, number\n",
            "with: one's, between, uncertainties, atlantic, user\n",
            "you: didn't, wouldn't, don't, we'll, we\n",
            "were: are, uncertainty, ericsson, they're, am\n",
            "win: lead, 92, megabits, 96, defeats\n",
            "those: persecution, customers, exactly, others, racist\n",
            "music: unlimited, legitimate, pictures, swap, gadget\n",
            "also: already, previously, repeatedly, reportedly, strongly\n",
            "international: 200m, meeting, rugby, policy, football\n",
            "best: counterparts, boxer, fastest, nomination, familiar\n",
            "down: together, back, slightly, she's, away\n",
            "too: very, so, less, enough, quite\n",
            "some: restrictions, practical, timely, obstacles, confusion\n",
            "through: spammers, housewives, batteries, â£0, upsetting\n",
            "mr: tony, ken, malcolm, gerhard, gordon\n",
            "fast: popularity, wireless, solutions, songs, minus\n",
            "road: stunning, exemptions, gatwick, soulful, einhoven\n",
            "bush: bush's, george, pozzebon, servat, reunites\n",
            "significant: key, similar, large, of, same\n",
            "reached: glory, successive, summer's, generation, atari\n",
            "serious: iaaf's, reasonable, awful, cj, effective\n",
            "wins: angry, chunk, approval, launch, suffered\n",
            "risk: odds, kind, imbalances, bupa, interact\n",
            "operating: financial, technology, environmental, bmw, provoked\n",
            "actually: we'll, she's, you're, you'll, anything\n",
            "deutsche: austria, austria's, reserve, central, michael's\n",
            "couple: ministry's, dermot, flic, emergence, kind\n",
            "asylum: genuinely, awaiting, determine, involved, quiksilver\n",
            "allowed: discover, modelled, unemployed, saves, trying\n",
            "davis: o'connell, fletcher, gleeson, uefa, simonetti\n",
            "mini: affordable, kooyong, freddie, lesotho, watchers\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 175ms/step - loss: 0.3820\n",
            "Epoch: 5/5 started\n",
            "   2233/Unknown \u001b[1m319s\u001b[0m 143ms/step - loss: 0.3541election: motors, november's, attorney, labour's, impending\n",
            "months: weeks, years, days, hours, least\n",
            "with: atlantic, uncertainties, one's, backwards, between\n",
            "you: we'll, you'll, tonight, didn't, i'll\n",
            "were: are, uncertainty, older, racist, dramatically\n",
            "win: eased, lead, radius, derby, clocking\n",
            "those: persecution, racist, exclude, affluent, customers\n",
            "music: unlimited, mp3, methods, pictures, gadget\n",
            "also: repeatedly, already, previously, strongly, originally\n",
            "international: 200m, scottish, football, rugby, meeting\n",
            "best: counterparts, boxer, category, nomination, feature\n",
            "down: aside, aggressively, slightly, incompatible, caps\n",
            "too: very, enough, less, worse, bigger\n",
            "some: restrictions, timely, specific, practical, login\n",
            "through: batteries, ak, bournemouth, nudity, upsetting\n",
            "mr: tony, sergio, gerhard, ken, malcolm\n",
            "fast: popularity, briton's, ones, risk, drivers\n",
            "road: stunning, exemptions, gatwick, constant, colour\n",
            "bush: bush's, pozzebon, george, servat, ec\n",
            "significant: substantial, key, reasonable, enormous, large\n",
            "reached: generation, successive, summer's, edged, glory\n",
            "serious: reasonable, effective, orders, congestion, huge\n",
            "wins: approval, suffered, finals, falls, finale\n",
            "risk: odds, encounters, testament, outcome, freedom\n",
            "operating: financial, technology, groundbreaking, environmental, atari\n",
            "actually: we'll, you'll, always, i'll, didn't\n",
            "deutsche: austria, austria's, energis, maroon, cebit\n",
            "couple: kind, debated, portion, ministry's, freedom\n",
            "asylum: genuinely, awaiting, quiksilver, determine, devastating\n",
            "allowed: modelled, weird, appears, discover, unwilling\n",
            "davis: gleeson, sean, o'connell, greenwood, ditto\n",
            "mini: affordable, watchers, kooyong, enforce, cm\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 143ms/step - loss: 0.3541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
        "\n",
        "    words_sorted = [None] + list(words_sorted)\n",
        "\n",
        "    pd.DataFrame(\n",
        "        model.get_layer(\"context_embedding\").get_weights()[0],\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"context_embedding.pkl\"))\n",
        "\n",
        "    pd.DataFrame(\n",
        "        model.get_layer(\"target_embedding\").get_weights()[0],\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"target_embedding.pkl\"))\n",
        "\n",
        "\n",
        "save_embeddings(skip_gram_model, tokenizer, n_vocab, save_dir='skipgram_embeddings')"
      ],
      "metadata": {
        "id": "SiYqYwATNImT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CBOW Algorithm**"
      ],
      "metadata": {
        "id": "gLrKb3-wWAXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cbow_grams(sequence, vocabulary_size,\n",
        "              window_size=4, negative_samples=1., shuffle=True,\n",
        "              categorical=False, sampling_table=None, seed=None):\n",
        "\n",
        "    targets, contexts, labels = [], [], []\n",
        "\n",
        "    for i, wi in enumerate(sequence):\n",
        "\n",
        "\n",
        "        if not wi or i < window_size or i + 1 > len(sequence)-window_size:\n",
        "            continue\n",
        "        if sampling_table is not None:\n",
        "            if sampling_table[wi] < random.random():\n",
        "                continue\n",
        "\n",
        "        window_start = max(0, i - window_size)\n",
        "        window_end = min(len(sequence), i + window_size + 1)\n",
        "\n",
        "        context_words = [wj for j, wj in enumerate(sequence[window_start:window_end]) if j+window_start != i]\n",
        "        target_word = wi\n",
        "\n",
        "        context_classes = tf.expand_dims(tf.constant(context_words, dtype=\"int64\"), 0)\n",
        "\n",
        "        negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "          true_classes=context_classes,\n",
        "          num_true=window_size * 2,\n",
        "          num_sampled=negative_samples,\n",
        "          unique=True,\n",
        "          range_max=vocabulary_size,\n",
        "          name=\"negative_sampling\")\n",
        "\n",
        "        # Build context and label vectors (for one target word)\n",
        "        negative_targets = negative_sampling_candidates.numpy().tolist()\n",
        "\n",
        "        target = [target_word] + negative_targets\n",
        "        label = [1] + [0]*negative_samples\n",
        "\n",
        "        # Append each element from the training example to global lists.\n",
        "        targets.extend(target)\n",
        "        contexts.extend([context_words]*(negative_samples+1))\n",
        "        labels.extend(label)\n",
        "\n",
        "    couples = list(zip(targets, contexts))\n",
        "\n",
        "    seed = random.randint(0, 10e6)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(couples)\n",
        "    random.seed(seed)\n",
        "    random.shuffle(labels)\n",
        "\n",
        "\n",
        "    return couples, labels\n",
        "\n",
        "\n",
        "window_size = 1 # How many words to consider left and right.\n",
        "\n",
        "\n",
        "inputs, labels = cbow_grams(\n",
        "    tokenizer.texts_to_sequences([\"I am going to the store\"])[0],\n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
        "    window_size=window_size, negative_samples=4, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "print(tokenizer.texts_to_sequences([\"I am going to the store\"]))\n",
        "i = 0\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    i += 1\n",
        "    print(f\"Input: {inp} ({[[tokenizer.index_word[wi] for wi in inp[1] ]] + [tokenizer.index_word[inp[0]] if inp[0] > 0 else None]})/ Label: {lbl}\")\n",
        "    #\n",
        "    if i >= 20:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfrGABV4WD66",
        "outputId": "b8da0dd9-68cc-4262-fcf9-a76347a1bb04"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[28, 428, 132, 3, 2, 1508]]\n",
            "Input: (14601, [132, 2]) ([['going', 'the'], \"cuckoo's\"])/ Label: 0\n",
            "Input: (662, [3, 1508]) ([['to', 'store'], 'shows'])/ Label: 0\n",
            "Input: (9134, [428, 3]) ([['am', 'to'], \"countries'\"])/ Label: 0\n",
            "Input: (842, [3, 1508]) ([['to', 'store'], 'numbers'])/ Label: 0\n",
            "Input: (354, [132, 2]) ([['going', 'the'], 'does'])/ Label: 0\n",
            "Input: (132, [428, 3]) ([['am', 'to'], 'going'])/ Label: 1\n",
            "Input: (5046, [28, 132]) ([['i', 'going'], 'neath'])/ Label: 0\n",
            "Input: (5976, [28, 132]) ([['i', 'going'], 'accepting'])/ Label: 0\n",
            "Input: (428, [28, 132]) ([['i', 'going'], 'am'])/ Label: 1\n",
            "Input: (9, [132, 2]) ([['going', 'the'], 'is'])/ Label: 0\n",
            "Input: (74, [28, 132]) ([['i', 'going'], 'she'])/ Label: 0\n",
            "Input: (84, [3, 1508]) ([['to', 'store'], 'do'])/ Label: 0\n",
            "Input: (3, [428, 3]) ([['am', 'to'], 'to'])/ Label: 0\n",
            "Input: (12, [3, 1508]) ([['to', 'store'], 'said'])/ Label: 0\n",
            "Input: (2243, [428, 3]) ([['am', 'to'], 'allows'])/ Label: 0\n",
            "Input: (1, [428, 3]) ([['am', 'to'], ''])/ Label: 0\n",
            "Input: (2, [3, 1508]) ([['to', 'store'], 'the'])/ Label: 1\n",
            "Input: (65, [28, 132]) ([['i', 'going'], 'when'])/ Label: 0\n",
            "Input: (3, [132, 2]) ([['going', 'the'], 'to'])/ Label: 1\n",
            "Input: (6, [132, 2]) ([['going', 'the'], 'a'])/ Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 1 # How many words to consider left and right.\n",
        "\n",
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    tokenizer.texts_to_sequences([news_stories[0][:150]])[0],\n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
        "    window_size=window_size, negative_samples=4, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "i = 0\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    i += 1\n",
        "    print(f\"Input: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl8BfIKWXSKD",
        "outputId": "1191dfb3-d0e0-47fd-c8d7-69c7705cfa1e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [126, 7] (['labour', 'in']) / Label: 1\n",
            "Input: [7, 126] (['in', 'labour']) / Label: 1\n",
            "Input: [7, 2795] (['in', 'constituency']) / Label: 1\n",
            "Input: [2795, 7] (['constituency', 'in']) / Label: 1\n",
            "Input: [2795, 630] (['constituency', 'race']) / Label: 1\n",
            "Input: [630, 2795] (['race', 'constituency']) / Label: 1\n",
            "Input: [630, 1016] (['race', 'row']) / Label: 1\n",
            "Input: [1016, 630] (['row', 'race']) / Label: 1\n",
            "Input: [1016, 787] (['row', \"labour's\"]) / Label: 1\n",
            "Input: [787, 1016] ([\"labour's\", 'row']) / Label: 1\n",
            "Input: [787, 661] ([\"labour's\", 'choice']) / Label: 1\n",
            "Input: [661, 787] (['choice', \"labour's\"]) / Label: 1\n",
            "Input: [661, 4] (['choice', 'of']) / Label: 1\n",
            "Input: [4, 661] (['of', 'choice']) / Label: 1\n",
            "Input: [4, 6] (['of', 'a']) / Label: 1\n",
            "Input: [6, 4] (['a', 'of']) / Label: 1\n",
            "Input: [6, 1094] (['a', 'white']) / Label: 1\n",
            "Input: [1094, 6] (['white', 'a']) / Label: 1\n",
            "Input: [1094, 2432] (['white', 'candidate']) / Label: 1\n",
            "Input: [2432, 1094] (['candidate', 'white']) / Label: 1\n",
            "Input: [2432, 8] (['candidate', 'for']) / Label: 1\n",
            "Input: [8, 2432] (['for', 'candidate']) / Label: 1\n",
            "Input: [8, 50] (['for', 'one']) / Label: 1\n",
            "Input: [50, 8] (['one', 'for']) / Label: 1\n",
            "Input: [50, 4] (['one', 'of']) / Label: 1\n",
            "Input: [4, 50] (['of', 'one']) / Label: 1\n",
            "Input: [4, 2] (['of', 'the']) / Label: 1\n",
            "Input: [2, 4] (['the', 'of']) / Label: 1\n",
            "Input: [2, 945] (['the', \"uk's\"]) / Label: 1\n",
            "Input: [945, 2] ([\"uk's\", 'the']) / Label: 1\n",
            "Input: [945, 113] ([\"uk's\", 'most']) / Label: 1\n",
            "Input: [113, 945] (['most', \"uk's\"]) / Label: 1\n",
            "Input: [113, 2391] (['most', 'multi']) / Label: 1\n",
            "Input: [2391, 113] (['multi', 'most']) / Label: 1\n",
            "Input: [2391, 5092] (['multi', 'racial']) / Label: 1\n",
            "Input: [5092, 2391] (['racial', 'multi']) / Label: 1\n",
            "Input: [5092, 2009] (['racial', 'seats']) / Label: 1\n",
            "Input: [2009, 5092] (['seats', 'racial']) / Label: 1\n",
            "Input: [2009, 5321] (['seats', 'proves']) / Label: 1\n",
            "Input: [5321, 2009] (['proves', 'seats']) / Label: 1\n",
            "Input: [5321, 2] (['proves', 'the']) / Label: 1\n",
            "Input: [2, 5321] (['the', 'proves']) / Label: 1\n",
            "Input: [2, 225] (['the', 'need']) / Label: 1\n",
            "Input: [225, 2] (['need', 'the']) / Label: 1\n",
            "Input: [225, 8] (['need', 'for']) / Label: 1\n",
            "Input: [8, 225] (['for', 'need']) / Label: 1\n",
            "Input: [8, 57] (['for', 'all']) / Label: 1\n",
            "Input: [57, 8] (['all', 'for']) / Label: 1\n",
            "Input: [57, 745] (['all', 'black']) / Label: 1\n",
            "Input: [745, 57] (['black', 'all']) / Label: 1\n",
            "Input: [745, 614] (['black', 'short']) / Label: 1\n",
            "Input: [614, 745] (['short', 'black']) / Label: 1\n",
            "Input: [113, 4778] (['most', 'cbi']) / Label: 0\n",
            "Input: [2391, 9390] (['multi', 'archive']) / Label: 0\n",
            "Input: [225, 17968] (['need', 'alltel']) / Label: 0\n",
            "Input: [1094, 21162] (['white', 'blushed']) / Label: 0\n",
            "Input: [5092, 12236] (['racial', '1935']) / Label: 0\n",
            "Input: [57, 21912] (['all', 'restating']) / Label: 0\n",
            "Input: [661, 26181] (['choice', 'marilyn']) / Label: 0\n",
            "Input: [4, 12669] (['of', 'mblox']) / Label: 0\n",
            "Input: [126, 18085] (['labour', 'manics']) / Label: 0\n",
            "Input: [787, 15823] ([\"labour's\", 'muggers']) / Label: 0\n",
            "Input: [745, 20410] (['black', 'vis']) / Label: 0\n",
            "Input: [5321, 32347] (['proves', 'cary']) / Label: 0\n",
            "Input: [745, 14530] (['black', 'interpol']) / Label: 0\n",
            "Input: [8, 18097] (['for', 'toppling']) / Label: 0\n",
            "Input: [5092, 24102] (['racial', 'basildon']) / Label: 0\n",
            "Input: [2432, 6226] (['candidate', 'gta']) / Label: 0\n",
            "Input: [2009, 13426] (['seats', 'crafted']) / Label: 0\n",
            "Input: [4, 25003] (['of', 'volcano']) / Label: 0\n",
            "Input: [2432, 561] (['candidate', 'main']) / Label: 0\n",
            "Input: [57, 9436] (['all', 'draws']) / Label: 0\n",
            "Input: [225, 29095] (['need', 'tantilising']) / Label: 0\n",
            "Input: [2795, 2468] (['constituency', 'sun']) / Label: 0\n",
            "Input: [5321, 30984] (['proves', 'perpetual']) / Label: 0\n",
            "Input: [8, 27258] (['for', 'tent']) / Label: 0\n",
            "Input: [2, 1425] (['the', 'living']) / Label: 0\n",
            "Input: [945, 28877] ([\"uk's\", 'maneuvers']) / Label: 0\n",
            "Input: [6, 23028] (['a', 'biljon']) / Label: 0\n",
            "Input: [7, 9139] (['in', 'spiritual']) / Label: 0\n",
            "Input: [8, 2909] (['for', '42']) / Label: 0\n",
            "Input: [630, 14188] (['race', 'acrobatic']) / Label: 0\n",
            "Input: [1016, 27102] (['row', 'auctioneers']) / Label: 0\n",
            "Input: [2, 13743] (['the', 'builders']) / Label: 0\n",
            "Input: [787, 17051] ([\"labour's\", 'wessels']) / Label: 0\n",
            "Input: [2795, 31568] (['constituency', 'mulyani']) / Label: 0\n",
            "Input: [50, 344] (['one', 'later']) / Label: 0\n",
            "Input: [2, 1168] (['the', 'code']) / Label: 0\n",
            "Input: [661, 18781] (['choice', 'fridays']) / Label: 0\n",
            "Input: [113, 10216] (['most', 'respective']) / Label: 0\n",
            "Input: [4, 26545] (['of', 'sexiest']) / Label: 0\n",
            "Input: [4, 10251] (['of', 'sweeping']) / Label: 0\n",
            "Input: [630, 16799] (['race', 'karolina']) / Label: 0\n",
            "Input: [8, 22577] (['for', 'sikhs']) / Label: 0\n",
            "Input: [614, 27550] (['short', 'hardcastle']) / Label: 0\n",
            "Input: [1016, 15643] (['row', 'idleness']) / Label: 0\n",
            "Input: [7, 17890] (['in', '1943']) / Label: 0\n",
            "Input: [1094, 31553] (['white', 'discontinued']) / Label: 0\n",
            "Input: [2, 4892] (['the', 'marriage']) / Label: 0\n",
            "Input: [6, 12631] (['a', 'syncing']) / Label: 0\n",
            "Input: [50, 24213] (['one', 'choked']) / Label: 0\n",
            "Input: [945, 20201] ([\"uk's\", 'depressant']) / Label: 0\n",
            "Input: [2391, 25289] (['multi', 'hazlewood']) / Label: 0\n",
            "Input: [2009, 1961] (['seats', 'losses']) / Label: 0\n",
            "Input: [113, 6333] (['most', 'attending']) / Label: 0\n",
            "Input: [2391, 25541] (['multi', 'ry']) / Label: 0\n",
            "Input: [225, 6731] (['need', 'funny']) / Label: 0\n",
            "Input: [1094, 31016] (['white', 'visakhapatnam']) / Label: 0\n",
            "Input: [5092, 3519] (['racial', 'stood']) / Label: 0\n",
            "Input: [57, 26025] (['all', 'gollum']) / Label: 0\n",
            "Input: [661, 13277] (['choice', 'sailed']) / Label: 0\n",
            "Input: [4, 23162] (['of', \"adere's\"]) / Label: 0\n",
            "Input: [126, 22511] (['labour', \"mail's\"]) / Label: 0\n",
            "Input: [787, 11676] ([\"labour's\", 'fellowship']) / Label: 0\n",
            "Input: [745, 24049] (['black', 'sacchi']) / Label: 0\n",
            "Input: [5321, 3246] (['proves', 'dogs']) / Label: 0\n",
            "Input: [745, 32290] (['black', 'willmean']) / Label: 0\n",
            "Input: [8, 12525] (['for', 'apprentice']) / Label: 0\n",
            "Input: [5092, 21296] (['racial', \"dumpsite'\"]) / Label: 0\n",
            "Input: [2432, 19851] (['candidate', 'ewoks']) / Label: 0\n",
            "Input: [2009, 19324] (['seats', 'limbs']) / Label: 0\n",
            "Input: [4, 8907] (['of', 'advertise']) / Label: 0\n",
            "Input: [2432, 2800] (['candidate', 'understood']) / Label: 0\n",
            "Input: [57, 31735] (['all', 'averages']) / Label: 0\n",
            "Input: [225, 11521] (['need', 'mori']) / Label: 0\n",
            "Input: [2795, 303] (['constituency', 'media']) / Label: 0\n",
            "Input: [5321, 17307] (['proves', 'stirring']) / Label: 0\n",
            "Input: [8, 14380] (['for', 'haka']) / Label: 0\n",
            "Input: [2, 7053] (['the', 'tci']) / Label: 0\n",
            "Input: [945, 4150] ([\"uk's\", 'predictions']) / Label: 0\n",
            "Input: [6, 16276] (['a', 'enacted']) / Label: 0\n",
            "Input: [7, 15285] (['in', 'tesco']) / Label: 0\n",
            "Input: [8, 577] (['for', 'rates']) / Label: 0\n",
            "Input: [630, 19560] (['race', 'vigilantism']) / Label: 0\n",
            "Input: [1016, 25011] (['row', 'merges']) / Label: 0\n",
            "Input: [2, 22067] (['the', 'dictatorships']) / Label: 0\n",
            "Input: [787, 17769] ([\"labour's\", 'contemplates']) / Label: 0\n",
            "Input: [2795, 19223] (['constituency', 'addicts']) / Label: 0\n",
            "Input: [50, 12741] (['one', 'drain']) / Label: 0\n",
            "Input: [2, 31998] (['the', \"toshitsune's\"]) / Label: 0\n",
            "Input: [661, 13311] (['choice', 'asbos']) / Label: 0\n",
            "Input: [113, 14693] (['most', 'mack']) / Label: 0\n",
            "Input: [4, 14316] (['of', 'bewildered']) / Label: 0\n",
            "Input: [4, 11890] (['of', 'alonso']) / Label: 0\n",
            "Input: [630, 15137] (['race', 'frames']) / Label: 0\n",
            "Input: [8, 29602] (['for', 'mainline']) / Label: 0\n",
            "Input: [614, 3920] (['short', 'lansdowne']) / Label: 0\n",
            "Input: [1016, 15845] (['row', 'shropshire']) / Label: 0\n",
            "Input: [7, 10018] (['in', 'ignorance']) / Label: 0\n",
            "Input: [1094, 8115] (['white', 'bassist']) / Label: 0\n",
            "Input: [2, 23802] (['the', \"stadium's\"]) / Label: 0\n",
            "Input: [6, 13551] (['a', 'distinction']) / Label: 0\n",
            "Input: [50, 4055] (['one', 'tindall']) / Label: 0\n",
            "Input: [945, 12371] ([\"uk's\", 'â£52']) / Label: 0\n",
            "Input: [2391, 21926] (['multi', 'refusals']) / Label: 0\n",
            "Input: [2009, 1581] (['seats', 'features']) / Label: 0\n",
            "Input: [113, 24569] (['most', 'milkman']) / Label: 0\n",
            "Input: [2391, 30888] (['multi', 'libyan']) / Label: 0\n",
            "Input: [225, 9873] (['need', 'pouwelse']) / Label: 0\n",
            "Input: [1094, 7325] (['white', 'heralded']) / Label: 0\n",
            "Input: [5092, 11002] (['racial', 'souped']) / Label: 0\n",
            "Input: [57, 8196] (['all', 'notebook']) / Label: 0\n",
            "Input: [661, 2975] (['choice', 'document']) / Label: 0\n",
            "Input: [4, 29484] (['of', 'hemin']) / Label: 0\n",
            "Input: [126, 5428] (['labour', 'merged']) / Label: 0\n",
            "Input: [787, 20403] ([\"labour's\", 'alencar']) / Label: 0\n",
            "Input: [745, 12822] (['black', 'suitor']) / Label: 0\n",
            "Input: [5321, 21766] (['proves', \"qur'aan\"]) / Label: 0\n",
            "Input: [745, 20727] (['black', \"on's\"]) / Label: 0\n",
            "Input: [8, 14250] (['for', 'bore']) / Label: 0\n",
            "Input: [5092, 7722] (['racial', 'diplomatic']) / Label: 0\n",
            "Input: [2432, 24405] (['candidate', \"athletic's\"]) / Label: 0\n",
            "Input: [2009, 13335] (['seats', 'assaults']) / Label: 0\n",
            "Input: [4, 14224] (['of', 'elimination']) / Label: 0\n",
            "Input: [2432, 18439] (['candidate', 'aborted']) / Label: 0\n",
            "Input: [57, 10412] (['all', 'tighten']) / Label: 0\n",
            "Input: [225, 11207] (['need', 'rocks']) / Label: 0\n",
            "Input: [2795, 26961] (['constituency', 'clowns']) / Label: 0\n",
            "Input: [5321, 25113] (['proves', 'dmx']) / Label: 0\n",
            "Input: [8, 4329] (['for', 'recorder']) / Label: 0\n",
            "Input: [2, 17980] (['the', 'lily']) / Label: 0\n",
            "Input: [945, 15961] ([\"uk's\", \"risk'\"]) / Label: 0\n",
            "Input: [6, 13402] (['a', 'dare']) / Label: 0\n",
            "Input: [7, 4119] (['in', 'warm']) / Label: 0\n",
            "Input: [8, 31571] (['for', 'sukarnoputri']) / Label: 0\n",
            "Input: [630, 24330] (['race', 'gelled']) / Label: 0\n",
            "Input: [1016, 21115] (['row', 'hospice']) / Label: 0\n",
            "Input: [2, 11325] (['the', 'motorists']) / Label: 0\n",
            "Input: [787, 7330] ([\"labour's\", 'dallas']) / Label: 0\n",
            "Input: [2795, 11313] (['constituency', 'heightened']) / Label: 0\n",
            "Input: [50, 6631] (['one', 'greenwood']) / Label: 0\n",
            "Input: [2, 16229] (['the', 'baltic']) / Label: 0\n",
            "Input: [661, 13162] (['choice', 'cohesion']) / Label: 0\n",
            "Input: [113, 6458] (['most', 'attraction']) / Label: 0\n",
            "Input: [4, 15388] (['of', 'sedans']) / Label: 0\n",
            "Input: [4, 27318] (['of', 'counterterrorism']) / Label: 0\n",
            "Input: [630, 18402] (['race', 'hustle']) / Label: 0\n",
            "Input: [8, 19832] (['for', 'degradation']) / Label: 0\n",
            "Input: [614, 30984] (['short', 'perpetual']) / Label: 0\n",
            "Input: [1016, 16504] (['row', 'spence']) / Label: 0\n",
            "Input: [7, 2471] (['in', 'bafta']) / Label: 0\n",
            "Input: [1094, 19926] (['white', 'hydraulic']) / Label: 0\n",
            "Input: [2, 14890] (['the', 'cates']) / Label: 0\n",
            "Input: [6, 19735] (['a', \"haves'\"]) / Label: 0\n",
            "Input: [50, 21003] (['one', 'missive']) / Label: 0\n",
            "Input: [945, 27957] ([\"uk's\", 'ooze']) / Label: 0\n",
            "Input: [2391, 13292] (['multi', 'rochdale']) / Label: 0\n",
            "Input: [2009, 15763] (['seats', 'europhiles']) / Label: 0\n",
            "Input: [113, 25916] (['most', 'blasphemy']) / Label: 0\n",
            "Input: [2391, 25831] (['multi', 'auteuil']) / Label: 0\n",
            "Input: [225, 11442] (['need', 'parton']) / Label: 0\n",
            "Input: [1094, 14797] (['white', 'adarsh']) / Label: 0\n",
            "Input: [5092, 22271] (['racial', 'centralises']) / Label: 0\n",
            "Input: [57, 9676] (['all', 'kenzie']) / Label: 0\n",
            "Input: [661, 16360] (['choice', 'ninety']) / Label: 0\n",
            "Input: [4, 2382] (['of', 'dublin']) / Label: 0\n",
            "Input: [126, 2366] (['labour', 'learning']) / Label: 0\n",
            "Input: [787, 17815] ([\"labour's\", \"cage's\"]) / Label: 0\n",
            "Input: [745, 1856] (['black', 'providing']) / Label: 0\n",
            "Input: [5321, 28149] (['proves', 'europay']) / Label: 0\n",
            "Input: [745, 30761] (['black', 'prioritised']) / Label: 0\n",
            "Input: [8, 7449] (['for', 'frame']) / Label: 0\n",
            "Input: [5092, 29973] (['racial', '671']) / Label: 0\n",
            "Input: [2432, 25583] (['candidate', 'connelly']) / Label: 0\n",
            "Input: [2009, 9945] (['seats', 'winemaker']) / Label: 0\n",
            "Input: [4, 8326] (['of', 'socially']) / Label: 0\n",
            "Input: [2432, 27607] (['candidate', \"life's\"]) / Label: 0\n",
            "Input: [57, 2219] (['all', 'outstanding']) / Label: 0\n",
            "Input: [225, 22994] (['need', 'precaution']) / Label: 0\n",
            "Input: [2795, 30629] (['constituency', 'armando']) / Label: 0\n",
            "Input: [5321, 31237] (['proves', 'catchphrase']) / Label: 0\n",
            "Input: [8, 16765] (['for', 'ruthless']) / Label: 0\n",
            "Input: [2, 6674] (['the', 'relied']) / Label: 0\n",
            "Input: [945, 30403] ([\"uk's\", 'pillars']) / Label: 0\n",
            "Input: [6, 18723] (['a', 'swimming']) / Label: 0\n",
            "Input: [7, 22845] (['in', 'vima']) / Label: 0\n",
            "Input: [8, 18137] (['for', 'updike']) / Label: 0\n",
            "Input: [630, 14261] (['race', 'jagielka']) / Label: 0\n",
            "Input: [1016, 9052] (['row', 'classed']) / Label: 0\n",
            "Input: [2, 2375] (['the', 'label']) / Label: 0\n",
            "Input: [787, 24721] ([\"labour's\", 'warbrick']) / Label: 0\n",
            "Input: [2795, 15744] (['constituency', 'temko']) / Label: 0\n",
            "Input: [50, 28544] (['one', 'expandable']) / Label: 0\n",
            "Input: [2, 4387] (['the', \"chancellor's\"]) / Label: 0\n",
            "Input: [661, 6108] (['choice', 'pierre']) / Label: 0\n",
            "Input: [113, 8248] (['most', 'heating']) / Label: 0\n",
            "Input: [4, 25583] (['of', 'connelly']) / Label: 0\n",
            "Input: [4, 3015] (['of', 'gross']) / Label: 0\n",
            "Input: [630, 10788] (['race', 'originality']) / Label: 0\n",
            "Input: [8, 2806] (['for', 'reflects']) / Label: 0\n",
            "Input: [614, 4267] (['short', 'honest']) / Label: 0\n",
            "Input: [1016, 123] (['row', 'may']) / Label: 0\n",
            "Input: [7, 22292] (['in', 'wallowing']) / Label: 0\n",
            "Input: [1094, 31393] (['white', 'congressmen']) / Label: 0\n",
            "Input: [2, 17014] (['the', 'kieran']) / Label: 0\n",
            "Input: [6, 12264] (['a', 'doren']) / Label: 0\n",
            "Input: [50, 6701] (['one', 'applying']) / Label: 0\n",
            "Input: [945, 2375] ([\"uk's\", 'label']) / Label: 0\n",
            "Input: [2391, 2341] (['multi', 'fake']) / Label: 0\n",
            "Input: [2009, 7585] (['seats', 'societies']) / Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4096 # Data points in a single batch\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "window_size=1 # We use a window size of 1 on either side of target word\n",
        "epochs = 5 # Number of epochs to train for\n",
        "negative_samples = 4 # Number of negative samples generated per example\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 250\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words as well\n",
        "np.random.seed(54321)\n",
        "random.seed(54321)\n",
        "\n",
        "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_term_ids = np.append(\n",
        "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
        "    axis=0\n",
        ")"
      ],
      "metadata": {
        "id": "Az5oOs-gXat7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "\n",
        "# Inputs; target input layer will have the final shape [None]\n",
        "# context will have [None, 2xwindow_size] shape\n",
        "input_1 = tf.keras.layers.Input(shape=())\n",
        "input_2 = tf.keras.layers.Input(shape=(window_size*2,))\n",
        "\n",
        "# Target and context embedding layers\n",
        "target_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
        ")\n",
        "\n",
        "context_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
        ")\n",
        "\n",
        "# Outputs of the target and context embedding lookups\n",
        "context_out = context_embedding_layer(input_2)\n",
        "target_out = target_embedding_layer(input_1)\n",
        "\n",
        "# Taking the mean over the all the context words to produce [None, embedding_size]\n",
        "mean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out)\n",
        "\n",
        "# Computing the dot product between the two\n",
        "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
        "\n",
        "cbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model')\n",
        "\n",
        "cbow_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam',\n",
        ")\n",
        "\n",
        "cbow_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "szrEDP3ZXdxz",
        "outputId": "c4039194-022a-4aee-a4a4-6d6346238319"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"cbow_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"cbow_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │      \u001b[38;5;34m4,142,080\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m4,142,080\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ context_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ target_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ target_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cbow_data_generator(sequences, window_size, batch_size, negative_samples):\n",
        "\n",
        "    rand_sequence_ids = np.arange(len(sequences))\n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "    for si in rand_sequence_ids:\n",
        "        inputs, labels = cbow_grams(\n",
        "            sequences[si],\n",
        "            vocabulary_size=n_vocab,\n",
        "            window_size=window_size,\n",
        "            negative_samples=negative_samples,\n",
        "            shuffle=True,\n",
        "            sampling_table=sampling_table,\n",
        "            seed=None\n",
        "        )\n",
        "\n",
        "        inputs_context, inputs_target, labels = np.array([inp[1] for inp in inputs]), np.array([inp[0] for inp in inputs]), np.array(labels).reshape(-1,1)\n",
        "\n",
        "        assert inputs_context.shape[0] == inputs_target.shape[0]\n",
        "        assert inputs_context.shape[0] == labels.shape[0]\n",
        "\n",
        "        #print(inputs_context.shape, inputs_target.shape, labels.shape)\n",
        "        for eg_id_start in range(0, inputs_context.shape[0], batch_size):\n",
        "\n",
        "            yield (\n",
        "                inputs_target[eg_id_start: min(eg_id_start+batch_size, inputs_target.shape[0])],\n",
        "                inputs_context[eg_id_start: min(eg_id_start+batch_size, inputs_context.shape[0]),:]\n",
        "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]"
      ],
      "metadata": {
        "id": "0jepmtGzXmBy"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer)\n",
        "\n",
        "for ei in range(epochs):\n",
        "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
        "    news_cbow_gen = cbow_data_generator(news_sequences, window_size, batch_size, negative_samples)\n",
        "    cbow_model.fit(\n",
        "        news_cbow_gen,\n",
        "        epochs=1,\n",
        "        callbacks=cbow_validation_callback,\n",
        "    )"
      ],
      "metadata": {
        "id": "88mcTUB3Xsfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_embeddings(cbow_model, tokenizer, n_vocab, save_dir='cbow_embeddings')"
      ],
      "metadata": {
        "id": "QZG6XcPaX7zU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}