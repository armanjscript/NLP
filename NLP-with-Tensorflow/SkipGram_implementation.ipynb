{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "%matplotlib inline\n",
        "!pip install adjustText\n",
        "import zipfile\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from six.moves.urllib.request import urlretrieve\n",
        "from sklearn.manifold import TSNE\n",
        "from adjustText import adjust_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uRT4CSp7NLu",
        "outputId": "f05a7ede-0051-4177-cb4c-8d27c61ec3a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adjustText\n",
            "  Downloading adjustText-1.2.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from adjustText) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from adjustText) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from adjustText) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->adjustText) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->adjustText) (1.16.0)\n",
            "Downloading adjustText-1.2.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: adjustText\n",
            "Successfully installed adjustText-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_data(url, data_dir):\n",
        "\n",
        "  os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "  filepath = os.path.join(data_dir, 'bbc-fulltext.zip')\n",
        "\n",
        "  if not os.path.exists(filepath):\n",
        "    print('Downloading file ...')\n",
        "    filename, _ = urlretrieve(url, filepath)\n",
        "  else:\n",
        "    print('File already exists')\n",
        "\n",
        "  extract_path = os.path.join(data_dir, 'bbc')\n",
        "  if not os.path.exists(extract_path):\n",
        "    with zipfile.ZipFile(\n",
        "        os.path.join(data_dir, 'bbc-fulltext.zip'),\n",
        "        'r'\n",
        "    ) as zipf:\n",
        "      zipf.extractall(data_dir)\n",
        "  else:\n",
        "    print('bbc-fulltext.zip has already been extracted')"
      ],
      "metadata": {
        "id": "kGF38Wg2LpAp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\n",
        "download_data(url, 'data')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MM3KiOtN3Ai",
        "outputId": "ed554df4-1ff4-48d3-f78a-16d8ae34c1b1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading file ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(data_dir):\n",
        "\n",
        "    # This will contain the full list of stories\n",
        "    news_stories = []\n",
        "\n",
        "    print(\"Reading files\")\n",
        "\n",
        "    i = 0 # Just used for printing progress\n",
        "    for root, dirs, files in os.walk(data_dir):\n",
        "\n",
        "        for fi, f in enumerate(files):\n",
        "\n",
        "            # We don't read the readme file\n",
        "            if 'README' in f:\n",
        "                continue\n",
        "\n",
        "            # Printing progress\n",
        "            i += 1\n",
        "            print(\".\"*i, f, end='\\r')\n",
        "\n",
        "            # Open the file\n",
        "            with open(os.path.join(root, f), encoding='latin-1') as f:\n",
        "\n",
        "                story = []\n",
        "                # Read all the lines\n",
        "                for row in f:\n",
        "\n",
        "                    story.append(row.strip())\n",
        "\n",
        "                # Create a single string with all the rows in the doc\n",
        "                story = ' '.join(story)\n",
        "                # Add that to the list\n",
        "                news_stories.append(story)\n",
        "\n",
        "        print('', end='\\r')\n",
        "\n",
        "    print(f\"\\nDetected {len(news_stories)} stories\")\n",
        "    return news_stories\n",
        "\n",
        "\n",
        "news_stories = read_data(os.path.join('data', 'bbc'))\n",
        "\n",
        "# Printing some stats and sample data\n",
        "print(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\n",
        "print('Example words (start): ',news_stories[0][:50])\n",
        "print('Example words (end): ',news_stories[-1][-50:])"
      ],
      "metadata": {
        "id": "Rf47cU9DOTGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80869722-eeb5-49d6-8443-3f43934bf972"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading files\n",
            "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................. 121.txt\n",
            "Detected 2225 stories\n",
            "865163 words found in the total news set\n",
            "Example words (start):  Labour in constituency race row  Labour's choice o\n",
            "Example words (end):  tion and low unemployment may be coming to an end.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words=None,\n",
        "    filters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower = True,\n",
        "    split = ' '\n",
        ")\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pbtd9uia8ab2",
        "outputId": "e6d1c183-f37b-49e7-ffc5-bd81d3a5c645"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_vocab = len(tokenizer.word_index.items()) + 1\n",
        "print(f\"Vocabulary size: {n_vocab}\")\n",
        "\n",
        "print(\"\\nWords at the top\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[:10]))\n",
        "print(\"\\nWords at the bottom\")\n",
        "print('\\t', dict(list(tokenizer.word_index.items())[-10:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENwqaP2M9e8n",
        "outputId": "97736bc7-4c6f-4980-844a-141a0f4eddcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 32360\n",
            "\n",
            "Words at the top\n",
            "\t {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\n",
            "\n",
            "Words at the bottom\n",
            "\t {'dz': 32350, 'bernd': 32351, 'weidensteiner': 32352, 'sinn': 32353, 'obstruct': 32354, 'avery': 32355, 'shenfeld': 32356, 'cibc': 32357, 'sohn': 32358, 'geopolitics': 32359}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(\n",
        "    num_words = 15000,\n",
        "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
        "    lower=True, split=' ', oov_token='',\n",
        ")\n",
        "tokenizer.fit_on_texts(news_stories)\n",
        "print(\"Data fitted on the tokenizer\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy6VS5Xe-SuN",
        "outputId": "ad4de616-35ab-4bc3-d77d-70c2d40dfd05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data fitted on the tokenizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Original: {news_stories[0][:100]}\")\n",
        "print(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go3LBI3P_Dzv",
        "outputId": "8b8f1284-4e0d-4c55-c7db-7ac771fd0e51"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: Labour in constituency race row  Labour's choice of a white candidate for one of the UK's most multi\n",
            "Sequence IDs: [126, 7, 2795, 630, 1016, 787, 661, 4, 6, 1094, 2432, 8, 50, 4, 2, 945, 113, 2391]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Converting all articles to word ID sequences**"
      ],
      "metadata": {
        "id": "GiDBLdfYAHaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news_sequences = tokenizer.texts_to_sequences(news_stories)"
      ],
      "metadata": {
        "id": "XDmOxn-KADWW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating skip-grams from the corpus**"
      ],
      "metadata": {
        "id": "LrgWbIVxAXVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_word_ids = news_sequences[0][:5]\n",
        "sample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\n",
        "print(f\"Sample phrase: {sample_phrase}\")\n",
        "print(f\"Sample word IDs: {sample_word_ids}\\n\")\n",
        "\n",
        "window_size = 1 # How many words to consider left and right.\n",
        "\n",
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=n_vocab,\n",
        "    window_size=window_size, negative_samples=1.0, shuffle=False,\n",
        "    categorical=False, sampling_table=None, seed=None\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Sample skip-grams\")\n",
        "\n",
        "for inp, lbl in zip(inputs, labels):\n",
        "    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) / Label: {lbl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZS3zFAPAZ1_",
        "outputId": "72f7a3b2-5ac2-45f7-cefe-b2a6053ad927"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample phrase: labour in constituency race row\n",
            "Sample word IDs: [126, 7, 2795, 630, 1016]\n",
            "\n",
            "Sample skip-grams\n",
            "\tInput: [126, 7] (['labour', 'in']) / Label: 1\n",
            "\tInput: [7, 126] (['in', 'labour']) / Label: 1\n",
            "\tInput: [7, 2795] (['in', 'constituency']) / Label: 1\n",
            "\tInput: [2795, 7] (['constituency', 'in']) / Label: 1\n",
            "\tInput: [2795, 630] (['constituency', 'race']) / Label: 1\n",
            "\tInput: [630, 2795] (['race', 'constituency']) / Label: 1\n",
            "\tInput: [630, 1016] (['race', 'row']) / Label: 1\n",
            "\tInput: [1016, 630] (['row', 'race']) / Label: 1\n",
            "\tInput: [126, 14684] (['labour', 'alabama']) / Label: 0\n",
            "\tInput: [630, 6407] (['race', 'totalling']) / Label: 0\n",
            "\tInput: [1016, 12663] (['row', 'compress']) / Label: 0\n",
            "\tInput: [7, 16213] (['in', 'â£947m']) / Label: 0\n",
            "\tInput: [2795, 21465] (['constituency', \"mackenzie's\"]) / Label: 0\n",
            "\tInput: [2795, 12498] (['constituency', 'fog']) / Label: 0\n",
            "\tInput: [7, 27145] (['in', 'pavlikowsky']) / Label: 0\n",
            "\tInput: [630, 23255] (['race', 'centimetre']) / Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating negative candidates**"
      ],
      "metadata": {
        "id": "x2UiRuokBnY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n",
        "    sample_word_ids,\n",
        "    vocabulary_size=len(tokenizer.word_index.items())+1,\n",
        "    window_size=window_size, negative_samples=0, shuffle=False,\n",
        ")\n",
        "\n",
        "inputs, labels = np.array(inputs), np.array(labels)\n",
        "\n",
        "negative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n",
        "    # A true context word that appears in the context of the target\n",
        "    true_classes=inputs[:1,1:], # [b, 1] sized tensor\n",
        "    num_true=1, # number of true words per example\n",
        "    num_sampled=10,\n",
        "    unique=True,\n",
        "    range_max=n_vocab,\n",
        "    name=\"negative_sampling\"\n",
        ")\n",
        "\n",
        "print(f\"Positive sample: {inputs[:1,1:]}\")\n",
        "print(f\"Negative samples: {negative_sampling_candidates}\")\n",
        "print(f\"true_expected_count: {true_expected_count}\")\n",
        "print(f\"sampled_expected_count: {sampled_expected_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRTY43xmBdim",
        "outputId": "953128c1-1442-4de6-9228-748dd7ce1b92"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive sample: [[7]]\n",
            "Negative samples: [   39  3099     3   336 20913     7    83   717  8621   109]\n",
            "true_expected_count: [[0.11341967]]\n",
            "sampled_expected_count: [2.37778574e-02 3.10580333e-04 2.14877039e-01 2.85319984e-03\n",
            " 4.60424235e-05 1.13419674e-01 1.13960411e-02 1.34022883e-03\n",
            " 1.11679241e-04 8.71457718e-03]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using tf.nn.sampled_softmax_loss()**"
      ],
      "metadata": {
        "id": "WxhRBcbM_tON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(n_vocab, sampling_factor=1e-05)\n",
        "print(sampling_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyg2K-SD_aIO",
        "outputId": "6c3b2168-6324-41a3-e654-68310948ab52"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.00315225 0.00315225 0.00547597 ... 1.         1.         1.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating data (positive + negative candidates)**"
      ],
      "metadata": {
        "id": "1bh2_mt7_3vV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None):\n",
        "\n",
        "    rand_sequence_ids = np.arange(len(sequences))\n",
        "    np.random.shuffle(rand_sequence_ids)\n",
        "\n",
        "\n",
        "    for si in rand_sequence_ids:\n",
        "\n",
        "        positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
        "            sequences[si],\n",
        "            vocabulary_size=vocab_size,\n",
        "            window_size=window_size,\n",
        "            negative_samples=0.0,\n",
        "            shuffle=False,\n",
        "            sampling_table=sampling_table,\n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "        targets, contexts, labels = [], [], []\n",
        "\n",
        "        for target_word, context_word in positive_skip_grams:\n",
        "            context_class = tf.expand_dims(tf.constant([context_word], dtype=\"int64\"), 1)\n",
        "\n",
        "            negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
        "              true_classes=context_class,\n",
        "              num_true=1,\n",
        "              num_sampled=negative_samples,\n",
        "              unique=True,\n",
        "              range_max=vocab_size,\n",
        "              name=\"negative_sampling\")\n",
        "\n",
        "            # Build context and label vectors (for one target word)\n",
        "            context = tf.concat(\n",
        "                [tf.constant([context_word], dtype='int64'), negative_sampling_candidates],\n",
        "                axis=0\n",
        "            )\n",
        "\n",
        "            label = tf.constant([1] + [0]*negative_samples, dtype=\"int64\")\n",
        "\n",
        "            # Append each element from the training example to global lists.\n",
        "            targets.extend([target_word]*(negative_samples+1))\n",
        "            contexts.append(context)\n",
        "            labels.append(label)\n",
        "\n",
        "        contexts, targets, labels = np.concatenate(contexts), np.array(targets), np.concatenate(labels)\n",
        "\n",
        "        assert contexts.shape[0] == targets.shape[0]\n",
        "        assert contexts.shape[0] == labels.shape[0]\n",
        "\n",
        "        # If seed is not provided generate a random one\n",
        "        if not seed:\n",
        "            seed = random.randint(0, 10e6)\n",
        "\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(contexts)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(targets)\n",
        "        np.random.seed(seed)\n",
        "        np.random.shuffle(labels)\n",
        "\n",
        "\n",
        "        for eg_id_start in range(0, contexts.shape[0], batch_size):\n",
        "            yield (\n",
        "                targets[eg_id_start: min(eg_id_start+batch_size, targets.shape[0])],\n",
        "                contexts[eg_id_start: min(eg_id_start+batch_size, contexts.shape[0])]\n",
        "            ), labels[eg_id_start: min(eg_id_start+batch_size, labels.shape[0])]\n",
        "\n",
        "\n",
        "news_skip_gram_gen = skip_gram_data_generator(\n",
        "    news_sequences, 4, 10, 5, n_vocab\n",
        ")\n",
        "\n",
        "for btc, bl in news_skip_gram_gen:\n",
        "\n",
        "    print(btc)\n",
        "    print(bl)\n",
        "\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ6X_pzK_0g-",
        "outputId": "ec742cfd-6135-46db-e945-e6d780db1df3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([ 1407,  3820,  8291,  2936,  3354, 14185,  8145,  5752,    26,\n",
            "        5767]), array([    2,  3592, 24048, 16115,  3820,  1769,   431,     6,    28,\n",
            "         191]))\n",
            "[0 0 0 0 1 0 1 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Skip -Gram Algorithm**"
      ],
      "metadata": {
        "id": "JRtIxnXJB19O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Hyperparameters\n",
        "\n",
        "batch_size = 4096 # Data points in a single batch\n",
        "\n",
        "embedding_size = 128 # Dimension of the embedding vector.\n",
        "\n",
        "window_size=1 # We use a window size of n on either side of target word\n",
        "negative_samples = 4 # Number of negative samples generated per example\n",
        "\n",
        "epochs = 5 # Number of epochs to train for\n",
        "\n",
        "# We pick a random validation set to sample nearest neighbors\n",
        "valid_size = 16 # Random set of words to evaluate similarity on.\n",
        "# We sample valid datapoints randomly from a large window without always being deterministic\n",
        "valid_window = 250\n",
        "\n",
        "# When selecting valid examples, we select some of the most frequent words as well as\n",
        "# some moderately rare words as well\n",
        "np.random.seed(54321)\n",
        "random.seed(54321)\n",
        "\n",
        "valid_term_ids = np.array(random.sample(range(valid_window), valid_size))\n",
        "valid_term_ids = np.append(\n",
        "    valid_term_ids, random.sample(range(1000, 1000+valid_window), valid_size),\n",
        "    axis=0\n",
        ")"
      ],
      "metadata": {
        "id": "-AaqeCSvB6H9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Model\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "# Inputs - skipgrams() function outputs target, context in that order\n",
        "# we will use the same order\n",
        "input_1 = tf.keras.layers.Input(shape=(), name='target')\n",
        "input_2 = tf.keras.layers.Input(shape=(), name='context')\n",
        "\n",
        "# Two embeddings layers are used one for the context and one for the target\n",
        "context_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='context_embedding'\n",
        ")\n",
        "target_embedding_layer = tf.keras.layers.Embedding(\n",
        "    input_dim=n_vocab, output_dim=embedding_size, name='target_embedding'\n",
        ")\n",
        "\n",
        "# Lookup outputs of the embedding layers\n",
        "target_out = target_embedding_layer(input_1)\n",
        "context_out = context_embedding_layer(input_2)\n",
        "\n",
        "# Computing the dot product between the two\n",
        "out = tf.keras.layers.Dot(axes=-1)([context_out, target_out])\n",
        "\n",
        "# Defining the model\n",
        "skip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='skip_gram_model')\n",
        "\n",
        "# Compiling the model\n",
        "skip_gram_model.compile(\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    optimizer='adam'\n",
        ")\n",
        "\n",
        "skip_gram_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "ouBjK2XcCZe-",
        "outputId": "726c4c3b-2421-400c-b11e-50c724244a9f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"skip_gram_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"skip_gram_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ context (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target (\u001b[38;5;33mInputLayer\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m)                 │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m4,142,080\u001b[0m │ context[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]          │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │      \u001b[38;5;34m4,142,080\u001b[0m │ target[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]           │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │              \u001b[38;5;34m0\u001b[0m │ context_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                           │                        │                │ target_embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ context (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                 │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ context_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ context[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]          │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ target_embedding          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,142,080</span> │ target[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]           │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)               │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ context_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                           │                        │                │ target_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,284,160\u001b[0m (31.60 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,284,160</span> (31.60 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculating Word Similarities\n",
        "\n",
        "class ValidationCallback(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n",
        "\n",
        "        self.valid_term_ids = valid_term_ids\n",
        "        self.model_with_embeddings = model_with_embeddings\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        \"\"\" Validation logic \"\"\"\n",
        "\n",
        "        # We will use context embeddings to get the most similar words\n",
        "        # Other strategies include: using target embeddings, mean embeddings after avaraging context/target\n",
        "        embedding_weights = self.model_with_embeddings.get_layer(\"context_embedding\").get_weights()[0]\n",
        "        normalized_embeddings = embedding_weights / np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n",
        "\n",
        "        # Get the embeddings corresponding to valid_term_ids\n",
        "        valid_embeddings = normalized_embeddings[self.valid_term_ids, :]\n",
        "\n",
        "        # Compute the similarity between valid_term_ids and all the embeddings\n",
        "        # V x d (d x D) => V x D\n",
        "        top_k = 5 # Top k items will be displayed\n",
        "        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n",
        "\n",
        "        # Invert similarity matrix to negative\n",
        "        # Ignore the first one because that would be the same word as the probe word\n",
        "        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: top_k+1]\n",
        "\n",
        "        # Print the output\n",
        "        for i, term_id in enumerate(valid_term_ids):\n",
        "\n",
        "            similar_word_str = ', '.join([self.tokenizer.index_word[j] for j in similarity_top_k[i, :] if j >= 1])\n",
        "            print(f\"{self.tokenizer.index_word[term_id]}: {similar_word_str}\")\n",
        "\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "Rl6nQdWzDc1W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the Skip-Gram Algorithm\n",
        "\n",
        "skipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\n",
        "\n",
        "for ei in range(epochs):\n",
        "\n",
        "    print(f\"Epoch: {ei+1}/{epochs} started\")\n",
        "\n",
        "    news_skip_gram_gen = skip_gram_data_generator(\n",
        "        news_sequences, window_size, batch_size, negative_samples, n_vocab\n",
        "    )\n",
        "\n",
        "    skip_gram_model.fit(\n",
        "        news_skip_gram_gen, epochs=1,\n",
        "        callbacks=skipgram_validation_callback,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1dQptTvGKIm",
        "outputId": "0cbfbc74-8922-44ce-ea6f-c7ef73e9a1f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/5 started\n",
            "   2233/Unknown \u001b[1m333s\u001b[0m 148ms/step - loss: 0.6367"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "election: attorney, anticipated, chong, tell, vido\n",
            "months: weeks, years, complaints, vote', affected\n",
            "with: 121, shadowing, statistical, beverley, against\n",
            "you: they, we, sure, what, do\n",
            "were: then, lost, only, mean, nothing\n",
            "win: likely, issue, serve, lot, trying\n",
            "those: doing, trying, lot, affected, mainstream\n",
            "music: way, trying, lot, concerned\n",
            "also: still, they, nothing, able, already\n",
            "international: came, religious, flooded, programmes, miscellaneous\n",
            "best: end, responsibility, surprised, forming\n",
            "down: able, can't, starting, now, machine\n",
            "too: taken, put, escape, fines, lot\n",
            "some: trying, case, side's, account\n",
            "through: me, boosted, possible, laptop, accusations\n",
            "mr: tony, said, gordon, charles, michael\n",
            "fast: understand, remake, affected, barring, take\n",
            "road: jameson, forsyth, proud, lifted, divided\n",
            "bush: replay, background, weeks', tally, moguls\n",
            "significant: according, kind, trying, spider, failure\n",
            "reached: trying, prepare, status, keen, heading\n",
            "serious: driven, others, see, jinshajiang, used\n",
            "wins: started, possibility, think, added, concerned\n",
            "risk: used, decided, happens, find, doing\n",
            "operating: collaboration, maybe, puts, edgware, force\n",
            "actually: idea, amicus', able, matter, switzerland\n",
            "deutsche: objected, insulted, prolong, illustrate, knows\n",
            "couple: kind, concerned, switzerland, lot, decided\n",
            "asylum: clocked, role, asked, dutchman, lead\n",
            "allowed: able, saves, find, likes, space\n",
            "davis: supremo, conservatives, rest, find, accompanied\n",
            "mini: built, switzerland, lot, trying, curb\n",
            "\n",
            "\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m333s\u001b[0m 148ms/step - loss: 0.6367\n",
            "Epoch: 2/5 started\n",
            "   2233/Unknown \u001b[1m355s\u001b[0m 159ms/step - loss: 0.4674election: attorney, impending, bmw, anticipated, drafted\n",
            "months: weeks, years, days, hours, number\n",
            "with: between, one's, bright, viotti's, recipients\n",
            "you: they, we, don't, wouldn't, didn't\n",
            "were: are, am, been, have, they're\n",
            "win: divide, lead, eased, switzerland, penalties\n",
            "those: 384, rocket, bother, abandoning, bysshe\n",
            "music: image, ofsted, arts, 2006', overture\n",
            "also: already, still, never, often, previously\n",
            "international: policy, great, meeting, rugby, land\n",
            "best: westlife, market's, jammed, joynt, chandratillake\n",
            "down: surge, roused, â£60bn, prototypes, mobiles\n",
            "too: voicing, score, retrieving, diatribe, liu\n",
            "some: ifpi, pensioners', pixies', offshore, disappear\n",
            "through: thirlwell, enthusiasts, 4x200m, mantilla, convince\n",
            "mr: tony, gordon, said, michael, jack\n",
            "fast: alerts, ramp, reissued, fingerprints, advancement\n",
            "road: academics, federation's, hitzlsperger's, they've, yard's\n",
            "bush: bush's, advocated, yellow, riot, crowther\n",
            "significant: according, thirty, presidency, alphabets, yugansk\n",
            "reached: 144, halt, directconnect, loudmouth, check\n",
            "serious: leveaux, rounding, â£104m, 1980\n",
            "wins: accountancy, imposed, wifi, crept, saviour\n",
            "risk: kind, lent, framework, credited, phpbb\n",
            "operating: transport, improving, another', dictate, balancing\n",
            "actually: demonstrations, thus, persuade, bloodthirsty, uniquely\n",
            "deutsche: initiated, reserve, best, german, union\n",
            "couple: kind, 1980, maldives, refusing, emergence\n",
            "asylum: genuinely, muna, avid, whelan, buzz\n",
            "allowed: apologise, able, done, bowers, leaning\n",
            "davis: heineken, fa, hopman, rascal, fletcher\n",
            "mini: kooyong, embattled, watchers, bears, uncompromising\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 159ms/step - loss: 0.4673\n",
            "Epoch: 3/5 started\n",
            "   2233/Unknown \u001b[1m378s\u001b[0m 169ms/step - loss: 0.4153election: attorney, anticipated, poster, november's, impending\n",
            "months: weeks, years, days, number, month\n",
            "with: one's, between, atlantic, fenice, viotti's\n",
            "you: didn't, don't, we, they, why\n",
            "were: are, am, been, they're, being\n",
            "win: lead, defeats, decade, eased, divide\n",
            "those: dre, others, browsers, exactly, rfu\n",
            "music: surround, walt, fearsome, contained, arts\n",
            "also: already, previously, often, repeatedly, never\n",
            "international: meeting, policy, 200m, world, rugby\n",
            "best: westlife, feature, fastest, familiar, counterparts\n",
            "down: together, back, comments, she's, penalty\n",
            "too: very, quite, enough, so, bit\n",
            "some: spice, eli, taxi, restrictions, compressed\n",
            "through: verbal, humans, anonymously, 'should, panic\n",
            "mr: tony, gordon, gerhard, malcolm, ken\n",
            "fast: popularity, undertaking, minus, alerts, wireless\n",
            "road: providing, detailed, monsters, your, reconstruction\n",
            "bush: bush's, george, pozzebon, chabal, gardner\n",
            "significant: reasonable, key, unprecedented, current, basic\n",
            "reached: fortuitous, idris, 144, anglicans, notorious\n",
            "serious: deeper, khatami's, rambo, awful, spamming\n",
            "wins: angry, launch, chunk, undertaking, suffered\n",
            "risk: plenty, parton, denunciation, homecoming, cadmium\n",
            "operating: astonishing, retains, elgindy's, technology, bulbs\n",
            "actually: we'll, you're, wasn't, shoppers, desperately\n",
            "deutsche: austria, central, austria's, english, takeover\n",
            "couple: kind, dermot, 86m, forsythe's, tainted\n",
            "asylum: genuinely, quiksilver, mate, kashmir, awaiting\n",
            "allowed: apologise, able, discover, pledged, fights\n",
            "davis: o'connell, uefa, fa, parisse, sean\n",
            "mini: kooyong, format, lesotho, embattled, avoided\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m378s\u001b[0m 169ms/step - loss: 0.4153\n",
            "Epoch: 4/5 started\n",
            "   2233/Unknown \u001b[1m390s\u001b[0m 175ms/step - loss: 0.3820election: attorney, november's, motors, anticipated, poster\n",
            "months: weeks, years, days, hours, number\n",
            "with: one's, between, uncertainties, atlantic, user\n",
            "you: didn't, wouldn't, don't, we'll, we\n",
            "were: are, uncertainty, ericsson, they're, am\n",
            "win: lead, 92, megabits, 96, defeats\n",
            "those: persecution, customers, exactly, others, racist\n",
            "music: unlimited, legitimate, pictures, swap, gadget\n",
            "also: already, previously, repeatedly, reportedly, strongly\n",
            "international: 200m, meeting, rugby, policy, football\n",
            "best: counterparts, boxer, fastest, nomination, familiar\n",
            "down: together, back, slightly, she's, away\n",
            "too: very, so, less, enough, quite\n",
            "some: restrictions, practical, timely, obstacles, confusion\n",
            "through: spammers, housewives, batteries, â£0, upsetting\n",
            "mr: tony, ken, malcolm, gerhard, gordon\n",
            "fast: popularity, wireless, solutions, songs, minus\n",
            "road: stunning, exemptions, gatwick, soulful, einhoven\n",
            "bush: bush's, george, pozzebon, servat, reunites\n",
            "significant: key, similar, large, of, same\n",
            "reached: glory, successive, summer's, generation, atari\n",
            "serious: iaaf's, reasonable, awful, cj, effective\n",
            "wins: angry, chunk, approval, launch, suffered\n",
            "risk: odds, kind, imbalances, bupa, interact\n",
            "operating: financial, technology, environmental, bmw, provoked\n",
            "actually: we'll, she's, you're, you'll, anything\n",
            "deutsche: austria, austria's, reserve, central, michael's\n",
            "couple: ministry's, dermot, flic, emergence, kind\n",
            "asylum: genuinely, awaiting, determine, involved, quiksilver\n",
            "allowed: discover, modelled, unemployed, saves, trying\n",
            "davis: o'connell, fletcher, gleeson, uefa, simonetti\n",
            "mini: affordable, kooyong, freddie, lesotho, watchers\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m390s\u001b[0m 175ms/step - loss: 0.3820\n",
            "Epoch: 5/5 started\n",
            "   2233/Unknown \u001b[1m319s\u001b[0m 143ms/step - loss: 0.3541election: motors, november's, attorney, labour's, impending\n",
            "months: weeks, years, days, hours, least\n",
            "with: atlantic, uncertainties, one's, backwards, between\n",
            "you: we'll, you'll, tonight, didn't, i'll\n",
            "were: are, uncertainty, older, racist, dramatically\n",
            "win: eased, lead, radius, derby, clocking\n",
            "those: persecution, racist, exclude, affluent, customers\n",
            "music: unlimited, mp3, methods, pictures, gadget\n",
            "also: repeatedly, already, previously, strongly, originally\n",
            "international: 200m, scottish, football, rugby, meeting\n",
            "best: counterparts, boxer, category, nomination, feature\n",
            "down: aside, aggressively, slightly, incompatible, caps\n",
            "too: very, enough, less, worse, bigger\n",
            "some: restrictions, timely, specific, practical, login\n",
            "through: batteries, ak, bournemouth, nudity, upsetting\n",
            "mr: tony, sergio, gerhard, ken, malcolm\n",
            "fast: popularity, briton's, ones, risk, drivers\n",
            "road: stunning, exemptions, gatwick, constant, colour\n",
            "bush: bush's, pozzebon, george, servat, ec\n",
            "significant: substantial, key, reasonable, enormous, large\n",
            "reached: generation, successive, summer's, edged, glory\n",
            "serious: reasonable, effective, orders, congestion, huge\n",
            "wins: approval, suffered, finals, falls, finale\n",
            "risk: odds, encounters, testament, outcome, freedom\n",
            "operating: financial, technology, groundbreaking, environmental, atari\n",
            "actually: we'll, you'll, always, i'll, didn't\n",
            "deutsche: austria, austria's, energis, maroon, cebit\n",
            "couple: kind, debated, portion, ministry's, freedom\n",
            "asylum: genuinely, awaiting, quiksilver, determine, devastating\n",
            "allowed: modelled, weird, appears, discover, unwilling\n",
            "davis: gleeson, sean, o'connell, greenwood, ditto\n",
            "mini: affordable, watchers, kooyong, enforce, cm\n",
            "\n",
            "\n",
            "\u001b[1m2233/2233\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 143ms/step - loss: 0.3541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_embeddings(model, tokenizer, vocab_size, save_dir):\n",
        "\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    _, words_sorted = zip(*sorted(list(tokenizer.index_word.items()), key=lambda x: x[0])[:vocab_size-1])\n",
        "\n",
        "    words_sorted = [None] + list(words_sorted)\n",
        "\n",
        "    pd.DataFrame(\n",
        "        model.get_layer(\"context_embedding\").get_weights()[0],\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"context_embedding.pkl\"))\n",
        "\n",
        "    pd.DataFrame(\n",
        "        model.get_layer(\"target_embedding\").get_weights()[0],\n",
        "        index = words_sorted\n",
        "    ).to_pickle(os.path.join(save_dir, \"target_embedding.pkl\"))\n",
        "\n",
        "\n",
        "save_embeddings(skip_gram_model, tokenizer, n_vocab, save_dir='skipgram_embeddings')"
      ],
      "metadata": {
        "id": "SiYqYwATNImT"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}